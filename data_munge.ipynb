{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "66c7eb72629e18a9d031742ed530b8e49c0aee74e57811e0d6a4f3cc7b309de0"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# CS3315 Final Project\n",
    "# Authors: Cameron Woods and Micky Hall \n",
    "\n",
    "## Introduction \n",
    "\n",
    "Throughout this notebook we will be attempting to create a model using supervised learning techniques that can accurately predict the price of an airbnb for a night. We will be using a dataset from kaggle that includes 226,029 rows. Each row represents an individual airbnb listing and includes 15 features and a label which will be discussed in depth at a later point in this notebook. \n",
    "\n",
    "### General Process \n",
    "\n",
    "We began this project by taking all of our unprocessed data and attempting to fit it into a bare bones model using linear regression to see how well it would perform. We then analysed the results and looked for reasons in the large skew in our predictions. We then went back and looked at the data as a whole and began to munge our data into a more palatable set for future models. We then checked for any increase in performance from our model. We then began to look into feature engineering and hyper parameter tuning for a greater fit. We slowly worked our way to a better model. We then decided to attempt to run our data in a neural network and see how well that would predict our label. All in all our models ended up predicting with a mse of %%%.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data loading and initial scrub. \n",
    "These features have been dropped because they can either be accurately captured \n",
    "in another feature, we felt they were irrelevant to our prediction, or they were too\n",
    "sparse to be able to munge\n",
    "'''\n",
    "\n",
    "import pandas as pd \n",
    "df = pd.read_csv(\"AB_US_2020.csv\")\n",
    "# For refrencing later \n",
    "unclean_df = df \n",
    "\n",
    "df = df.drop([\"name\",\"host_name\", \"neighbourhood_group\",\"city\",\"neighbourhood\",\n",
    "                \"last_review\",\"id\"],axis=1)\n",
    "\n",
    "df[\"reviews_per_month\"] = df[\"reviews_per_month\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "One hot encode the room type feature to be able to represent the differnt type\n",
    "of property you can rent\n",
    "'''\n",
    "\n",
    "def oneHot(category, hot):\n",
    "    if category == hot:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "dict={}\n",
    "for room in df['room_type'].tolist():\n",
    "    dict[room]=1\n",
    "    \n",
    "for room in dict.keys():\n",
    "    df[room] = df['room_type'].apply(oneHot, hot=room)\n",
    "\n",
    "df = df.drop(['room_type'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#drop zeros and negative prices, if any\n",
    "df = df[df.price > 75]\n",
    "#drop highest price, likely an outlier\n",
    "df = df[df.price < 200]\n",
    "\n",
    "X = df.drop([\"price\"],axis=1)\n",
    "y = df[\"price\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "sc = StandardScaler()\n",
    "#X_train = sc.fit_transform(X_train)\n",
    "#X_val = sc.transform(X_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "32.913017358916726\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "sgd_reg = LinearRegression()\n",
    "sgd_reg.fit(X_train, y_train) \n",
    "y_val_predict = sgd_reg.predict(X_val)\n",
    "val_error = sqrt(mean_squared_error(y_val, y_val_predict))\n",
    "print(val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The RMSE for our Random Forest Regressor is 150130.1425196851\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "\n",
    "forest.fit(X_train, y_train)\n",
    "y_val_predict_forest = forest.predict(X_val)\n",
    "val_error = sqrt(mean_squared_error(y_val, y_val_predict))\n",
    "print(\"The RMSE for our Random Forest Regressor is {}\".format(val_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"AB_US_2020.csv\")\n",
    "del df2[\"name\"]\n",
    "del df2[\"host_name\"]\n",
    "del df2[\"neighbourhood_group\"]\n",
    "del df2[\"city\"]\n",
    "del df2[\"neighbourhood\"]\n",
    "del df2[\"last_review\"]\n",
    "\n",
    "dict2={}\n",
    "for room in df2['room_type'].tolist():\n",
    "    dict2[room]=1\n",
    "    \n",
    "for room in dict2.keys():\n",
    "    df2[room] = df2['room_type'].apply(oneHot, hot=room)\n",
    "\n",
    "df2 = df2.drop(['room_type'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop zeros and negative prices, if any\n",
    "df2 = df2[df2.price > 0]\n",
    "#drop highest price, likely an outlier\n",
    "df2 = df2[df2.price < 24999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo basic linear regression\n",
    "\n",
    "df2[\"reviews_per_month\"]=df2[\"reviews_per_month\"].fillna(0)\n",
    "\n",
    "df2.to_csv(\"munge_plus_reviewspermonth.csv\")\n",
    "\n",
    "X2 = df2.drop([\"price\"],axis=1)\n",
    "y2 = df2[\"price\"]\n",
    "X2_train, X2_val, y2_train, y2_val = train_test_split(X2, y2, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "sc = StandardScaler()\n",
    "X2_train = sc.fit_transform(X2_train)\n",
    "X2_val = sc.transform(X2_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'X2_train' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7ad4174aaff9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0msgd_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0msgd_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX2_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my2_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0my2_val_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msgd_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX2_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mval_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my2_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my2_val_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X2_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "sgd_reg = LinearRegression()\n",
    "sgd_reg.fit(X2_train, y2_train) \n",
    "y2_val_predict = sgd_reg.predict(X2_val)\n",
    "val_error = sqrt(mean_squared_error(y2_val, y2_val_predict))\n",
    "print(val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "371.9635192644459\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest = RandomForestRegressor(n_estimators=80, min_samples_leaf=2, max_features=\"log2\", bootstrap=False)\n",
    "\n",
    "forest.fit(X_train, y_train)\n",
    "y_val_predict_forest = forest.predict(X_val)\n",
    "print(np.sqrt(mean_squared_error(y_val,y_val_predict_forest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:  6.5min finished\n",
      "424.6918635368347\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import time\n",
    "n_estimators = [int(x) for x in np.arange(start=10, stop = 100, step = 10)]\n",
    "max_features = [\"log2\"]\n",
    "min_samples_leaf = [2]\n",
    "bootstrap = [False]\n",
    "max_depth = [100,150,200,250,300,None]\n",
    "random_grid = {\"n_estimators\": n_estimators,\n",
    "               \"max_features\": max_features,\n",
    "               \"min_samples_leaf\": min_samples_leaf,\n",
    "               \"bootstrap\": bootstrap,\n",
    "               \"max_depth\":max_depth}\n",
    "\n",
    "rand = RandomForestRegressor()\n",
    "start = time.time()\n",
    "rand_search = RandomizedSearchCV(estimator = rand, param_distributions=random_grid, n_iter = 15, cv=5, verbose =2, random_state=42, n_jobs=-1)\n",
    "rand_search.fit(X_train,y_train)\n",
    "rand_search.best_params_\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'n_estimators': 80, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': None, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "print(rand_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}