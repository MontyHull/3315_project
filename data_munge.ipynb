{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# CS3315 Final Project\n",
    "# Authors: Cameron Woods and Micky Hall \n",
    "\n",
    "## Introduction \n",
    "\n",
    "Throughout this notebook we will be attempting to create a model using supervised learning techniques that can accurately predict the price of an airbnb for a night. We will be using a dataset from kaggle that includes 226,029 rows. Each row represents an individual airbnb listing and includes 15 features and a label which will be discussed in depth at a later point in this notebook. \n",
    "\n",
    "### General Process \n",
    "\n",
    "We began this project by taking all of our data and lightly processing it and attempting to fit it into a bare bones model using linear regression to see how well it would perform. We then analysed the results and looked for reasons in the large skew in our predictions. We then went back and looked at the data as a whole and began to munge our data into a more palatable set for future models. We then checked for any increase in performance from our model. We then began to look into feature engineering and hyper parameter tuning for a greater fit. We slowly worked our way to a better model. We then decided to attempt to run our data in a neural network and see how well that would predict our label. All in all our models ended up predicting with a mse of %%%.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data loading and initial scrub. \n",
    "These features have been dropped because they can either be accurately captured \n",
    "in another feature, we felt they were irrelevant to our prediction, or they were too\n",
    "sparse to be able to munge\n",
    "'''\n",
    "\n",
    "import pandas as pd \n",
    "df = pd.read_csv(\"AB_US_2020.csv\")\n",
    "# For refrencing later \n",
    "unclean_df = df \n",
    "\n",
    "df = df.drop([\"name\",\"host_name\", \"neighbourhood_group\",\"city\",\"neighbourhood\",\n",
    "                \"last_review\",\"id\"],axis=1)\n",
    "\n",
    "df[\"reviews_per_month\"] = df[\"reviews_per_month\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "One hot encode the room type feature to be able to represent the differnt type\n",
    "of property you can rent\n",
    "'''\n",
    "\n",
    "def oneHot(category, hot):\n",
    "    if category == hot:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "dict={}\n",
    "for room in df['room_type'].tolist():\n",
    "    dict[room]=1\n",
    "    \n",
    "for room in dict.keys():\n",
    "    df[room] = df['room_type'].apply(oneHot, hot=room)\n",
    "\n",
    "df = df.drop(['room_type'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#drop zeros and negative prices, if any\n",
    "df = df[df.price > 0]\n",
    "#drop highest price, likely an outlier\n",
    "df = df[df.price < 24999]\n",
    "\n",
    "# Split data into features and label \n",
    "X = df.drop([\"price\"],axis=1)\n",
    "y = df[\"price\"]\n",
    "\n",
    "# Create a training and a validation set of data. 80/20 split \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Scale the data for better predictions \n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_val = sc.transform(X_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "25639\n0\n56485.0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "count    210164.000000\n",
       "mean        142.301136\n",
       "std          95.469315\n",
       "min          21.000000\n",
       "25%          75.000000\n",
       "50%         115.000000\n",
       "75%         185.000000\n",
       "max         499.000000\n",
       "Name: price, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "below50 = df[df.price <= 50]\n",
    "above500 = df[df.price >= 500]\n",
    "print(len(below50))\n",
    "print(len(above500))\n",
    "print(225940/4)\n",
    "y.describe()\n"
   ]
  },
  {
   "source": [
    "## Initial Cleaning of Data\n",
    "\n",
    "We began with a dataset that contained the attributes:\n",
    "\n",
    "id, name, host_id, host_name, neighbourhood_group, neighbourhood, latitude, longitude, room_type, price, minimum_nights, number_of_reviews, last_review, reviews_per_month, calculated_host_listings_count, availability_365, and city. \n",
    "\n",
    "After reviewing the data set we decided that it would be best to drop name, host_name, neighbourhood_group, city, neighbourhood, last_review, and id. We dropped name and host_name because they are similar on many dissimilar listings and the value they represent is better caputred in the host_id. We dropped neighbourhood_group, city, and neighbourhood because many of these values were missing, and they can also be represented by the latitude and longitude values given. We dropped last review because it was just the last review of the property which would require us to do some form of semantic analysis to convert to a meaningful attribute. And finally we dropped id because it was just a unique identifier for each listing that held no real value for the models. \n",
    "\n",
    "After dropping these attributes we one hot encoded the room_type attribute so that all property types could be represented, and we filled in all empty values in the reviews_per_month since an empty value is likely to represent no reviews. \n",
    "\n",
    "After dropping and correcting our values we split our labels and features apart and then split them into training and validation sets and then scale them for our models. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The RMSE for our Linear Regression Model is 504.1372989338935\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "sgd_reg = LinearRegression()\n",
    "sgd_reg.fit(X_train, y_train) \n",
    "y_val_predict = sgd_reg.predict(X_val)\n",
    "val_error = sqrt(mean_squared_error(y_val, y_val_predict))\n",
    "print(\"The RMSE for our Linear Regression Model is {}\".format(val_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "\n",
    "forest.fit(X_train, y_train)\n",
    "y_val_predict_forest = forest.predict(X_val)\n",
    "val_error = sqrt(mean_squared_error(y_val, y_val_predict_forest))\n",
    "print(\"The RMSE for our Random Forest Regressor is {}\".format(val_error))"
   ]
  },
  {
   "source": [
    "## Initial running of models and light analysis  \n",
    "\n",
    "WeAfter lightly cleaning our data we run it through a Linear Regressor and a Random Forrest Regressowithout any hyperparameter tuning and ended up with: \n",
    "\n",
    "RMSE of Linear Regressor = 504.1366\n",
    "RMSE of Random Forest Regressor = 379.4682 \n",
    "\n",
    "When we initially look at this it seems that we are making decent predictions considering we our predicting within 1 standard deviation of error. However, when you look at the data our 75th percentile starts at a price of around 200, so we are grossly over predicting for most of our data. From here we can start to look at the data and munge it some more to try to get better estimates. r "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}