{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# CS3315 Final Project\n",
    "# Authors: Cameron Woods and Micky Hall \n",
    "\n",
    "## Introduction \n",
    "\n",
    "Throughout this notebook we will be attempting to create a model using supervised learning techniques that can accurately predict the price of an airbnb for a night. We will be using a dataset from kaggle that includes 226,029 rows. Each row represents an individual airbnb listing and includes 15 features and a label which will be discussed in depth at a later point in this notebook. \n",
    "\n",
    "### General Process \n",
    "\n",
    "\n",
    "We began this project by taking all of our data and lightly processing it and attempting to fit it into a bare bones model using linear regression to see how well it would perform. We then analysed the results and looked for reasons in the large skew in our predictions. We then went back and looked at the data as a whole and began to munge our data into a more palatable set for future models. We then checked for any increase in performance from our model. We then began to look into feature engineering and hyper parameter tuning for a greater fit. We slowly worked our way to a better model. We then decided to attempt to run our data in a neural network and see how well that would predict our label. All in all our models ended up predicting with a mse of %%%.   "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "import tensorflow as tf \n",
    "import  tensorflow.keras as keras \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data loading and initial scrub. \n",
    "These features have been dropped because they can either be accurately captured \n",
    "in another feature, we felt they were irrelevant to our prediction, or they were too\n",
    "sparse to be able to munge\n",
    "'''\n",
    "df = pd.read_csv(\"AB_US_2020.csv\")\n",
    "# For refrencing later \n",
    "unclean_df = df \n",
    "\n",
    "df = df.drop([\"name\",\"host_name\",\"city\",\"neighbourhood\",\n",
    "                \"last_review\",\"id\",\"neighbourhood_group\"],axis=1)\n",
    "\n",
    "df[\"reviews_per_month\"] = df[\"reviews_per_month\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "One hot encode the room type feature to be able to represent the differnt type\n",
    "of property you can rent\n",
    "'''\n",
    "def oneHot(category, hot):\n",
    "    if category == hot:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "dict={}\n",
    "for room in df['room_type'].tolist():\n",
    "    dict[room]=1\n",
    "    \n",
    "for room in dict.keys():\n",
    "    df[room] = df['room_type'].apply(oneHot, hot=room)\n",
    "\n",
    "df = df.drop(['room_type'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of our current dataset = 225940\nShape of the original dataset = 226029\nSo we have only dropped 89 rows, which is 0.03937547836782006 percent of our data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#drop zeros and negative prices, if any\n",
    "df = df[df.price > 0]\n",
    "#drop highest price, likely an outlier\n",
    "df = df[df.price < 24999]\n",
    "\n",
    "def data_dump(df):\n",
    "    X = df.drop([\"price\"],axis=1)\n",
    "    y = df[\"price\"]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_val = sc.transform(X_val)\n",
    "    return X,y,X_train,y_train,X_val,y_val\n",
    "def data_loss(original_df, current_df):\n",
    "    orig = original_df.shape[0]\n",
    "    clean = current_df.shape[0]\n",
    "    dropped = orig-clean\n",
    "    perc = (dropped/orig) *100\n",
    "    print(\"Shape of our current dataset = {}\".format(clean))\n",
    "    print(\"Shape of the original dataset = {}\".format(orig))\n",
    "    print(\"So we have only dropped {} rows, which is {} percent of our data\".format(dropped,perc))\n",
    "X,y,X_train,y_train,X_val,y_val = data_dump(df)\n",
    "data_loss(unclean_df,df)"
   ]
  },
  {
   "source": [
    "## Initial Cleaning of Data\n",
    "\n",
    "We began with a dataset that contained the attributes:\n",
    "\n",
    "id, name, host_id, host_name, neighbourhood_group, neighbourhood, latitude, longitude, room_type, price, minimum_nights, number_of_reviews, last_review, reviews_per_month, calculated_host_listings_count, availability_365, and city. \n",
    "\n",
    "After reviewing the data set we decided that it would be best to drop name, host_name, neighbourhood_group, city, neighbourhood, last_review, and id. We dropped name and host_name because they are similar on many dissimilar listings and the value they represent is better caputred in the host_id. We dropped neighbourhood_group, city, and neighbourhood because many of these values were missing, and they can also be represented by the latitude and longitude values given. We dropped last review because it was just the last review of the property which would require us to do some form of semantic analysis to convert to a meaningful attribute. And finally we dropped id because it was just a unique identifier for each listing that held no real value for the models. \n",
    "\n",
    "After dropping these attributes we one hot encoded the room_type attribute so that all property types could be represented, and we filled in all empty values in the reviews_per_month since an empty value is likely to represent no reviews. \n",
    "\n",
    "After dropping and correcting our values we split our labels and features apart and then split them into training and validation sets and then scale them for our models. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor_tester(reg, X_train, y_train, X_val,y_val,y, name,fit=True):\n",
    "    if(fit==True):\n",
    "        reg.fit(X_train, y_train) \n",
    "    y_val_predict = reg.predict(X_val)\n",
    "    RMSE = sqrt(mean_squared_error(y_val, y_val_predict))\n",
    "    MAE = mean_absolute_error(y_val,y_val_predict)\n",
    "    r2 = r2_score(y_val,y_val_predict)\n",
    "    mean = y.describe()[\"mean\"]\n",
    "    std_dev = y.describe()[\"std\"]\n",
    "    print(\"For our {} Regressor the RMSE is {}, MAE is {}, and r2 is {}\".format(name,RMSE,MAE,r2))\n",
    "    print(\"The current mean value for our label is {} and a single standard deviation is {}\".format(mean,std_dev))\n",
    "    return y_val_predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = LinearRegression()\n",
    "lin_predict = regressor_tester(lin,X_train,y_train,X_val,y_val,y,\"Linear\")\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest_predict = regressor_tester(forest,X_train,y_train,X_val,y_val,y,\"Random Forest\")"
   ]
  },
  {
   "source": [
    "## Initial running of models and light analysis  \n",
    "\n",
    "\n",
    "After lightly cleaning our data we run it through a Linear Regressor and a Random Forrest Regressowithout any hyperparameter tuning and ended up with: \n",
    "\n",
    "RMSE of Linear Regressor = 504.1366\n",
    "RMSE of Random Forest Regressor = 379.4682 \n",
    "\n",
    "When we initially look at this it seems that we are making decent predictions considering we our predicting within 1 standard deviation of error. However, when you look at the data our 75th percentile starts at a price of around 200, so we are grossly over predicting for most of our data. From here we can start to look at the data and munge it some more to try to get better estimates."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_df = pd.DataFrame({\"Actual\":y_val,\"Predict\":lin_predict})\n",
    "#sns.lmplot(x=\"Actual\",y=\"Prediction Error\",data=pred_df)\n",
    "#pred_df = pd.DataFrame({\"Actual\":y_val,\"Predict\":forest_predict})\n",
    "#sns.lmplot(x=\"Actual\",y=\"Prediction Error\",data=pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 12,12\n",
    "sns.heatmap(df.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.histplot(data=df, x=\"price\",kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop higher priced listings \n",
    "df = df[df.price < 2500]\n",
    "#sns.histplot(data=df, x=\"price\",kde=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.price < 1000]\n",
    "#sns.histplot(data=df, x=\"price\",kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.rcParams[\"figure.figsize\"] = 12,12\n",
    "#sns.heatmap(df.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after having dropped prices > 1000\n",
    "X,y,X_train,y_train,X_val,y_val = data_dump(df)\n",
    "data_loss(unclean_df,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_predict = regressor_tester(lin_reg,X_train,y_train,X_val,y_val,y,\"Linear\")\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest_predict = regressor_tester(forest,X_train,y_train,X_val,y_val,y,\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_df = pd.DataFrame({\"Actual\":y_val,\"Predict\":lin_y_val_predict})\n",
    "#sns.lmplot(x=\"Actual\",y=\"Prediction Error\",data=pred_df)\n",
    "#pred_df = pd.DataFrame({\"Actual\":y_val,\"Predict\":forest_y_val_predict})\n",
    "#sns.lmplot(x=\"Actual\",y=\"Prediction Error\",data=pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"minimum_nights\"],axis=1)\n",
    "X,y,X_train,y_train,X_val,y_val = data_dump(df)\n",
    "data_loss(unclean_df,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_predict = regressor_tester(lin_reg,X_train,y_train,X_val,y_val,y,\"Linear\")\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest_predict = regressor_tester(forest,X_train,y_train,X_val,y_val,y,\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"host_id\"],axis=1)\n",
    "X,y,X_train,y_train,X_val,y_val = data_dump(df)\n",
    "data_loss(unclean_df,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_predict = regressor_tester(lin_reg,X_train,y_train,X_val,y_val,y,\"Linear\")\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest_predict = regressor_tester(forest,X_train,y_train,X_val,y_val,y,\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of our current dataset = 214702\nShape of the original dataset = 226029\nSo we have only dropped 11327 rows, which is 5.011303859239301 percent of our data\n"
     ]
    }
   ],
   "source": [
    "df = df[df.price < 600]\n",
    "df = df.drop([\"Hotel room\"],axis=1)\n",
    "X,y,X_train,y_train,X_val,y_val = data_dump(df)\n",
    "data_loss(unclean_df,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 214702 entries, 0 to 226028\nData columns (total 12 columns):\n #   Column                          Non-Null Count   Dtype  \n---  ------                          --------------   -----  \n 0   Entire home/apt                 214702 non-null  int64  \n 1   Private room                    214702 non-null  int64  \n 2   Shared room                     214702 non-null  int64  \n 3   availability_365                214702 non-null  int64  \n 4   calculated_host_listings_count  214702 non-null  int64  \n 5   host_id                         214702 non-null  int64  \n 6   latitude                        214702 non-null  float64\n 7   longitude                       214702 non-null  float64\n 8   minimum_nights                  214702 non-null  int64  \n 9   number_of_reviews               214702 non-null  int64  \n 10  price                           214702 non-null  int64  \n 11  reviews_per_month               214702 non-null  float64\ndtypes: float64(3), int64(9)\nmemory usage: 21.3 MB\n"
     ]
    }
   ],
   "source": [
    "cols = list(df.columns)\n",
    "cols.sort() \n",
    "df = df[cols]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of our current dataset = 214702\n",
      "Shape of the original dataset = 226029\n",
      "So we have only dropped 11327 rows, which is 5.011303859239301 percent of our data\n",
      "For our Linear Regressor the RMSE is 95.44174486620469, MAE is 68.10286878885323, and r2 is 0.21046828039987608\n",
      "The current mean value for our label is 148.66011029240528 and a single standard deviation is 107.94989078801439\n",
      "For our Random Forest Regressor the RMSE is 78.87424300602446, MAE is 52.9542321146689, and r2 is 0.4607834047603928\n",
      "The current mean value for our label is 148.66011029240528 and a single standard deviation is 107.94989078801439\n"
     ]
    }
   ],
   "source": [
    "X,y,X_train,y_train,X_val,y_val = data_dump(df)\n",
    "data_loss(unclean_df,df)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_predict = regressor_tester(lin_reg,X_train,y_train,X_val,y_val,y,\"Linear\")\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest_predict = regressor_tester(forest,X_train,y_train,X_val,y_val,y,\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert neighbourhood into label encoded featuresS\n",
    "labelencoder = LabelEncoder()\n",
    "df[\"neighbourhood\"] = unclean_df[\"neighbourhood\"]\n",
    "df[\"neighbourhood\"] = labelencoder.fit_transform(df[\"neighbourhood\"])\n",
    "X,y,X_train,y_train,X_val,y_val = data_dump(df)\n",
    "data_loss(unclean_df,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_predict = regressor_tester(lin_reg,X_train,y_train,X_val,y_val,y,\"Linear\")\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest_predict = regressor_tester(forest,X_train,y_train,X_val,y_val,y,\"Random Forest\")"
   ]
  },
  {
   "source": [
    "## After Data Analysis "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_estimators = list(range(10,101,10))\n",
    "max_features = [\"log2\",\"auto\",\"sqrt\"]\n",
    "min_samples_leaf = [1,2,4,6]\n",
    "bootstrap = [False,True]\n",
    "max_depth = [100,150,200,250,300,None]\n",
    "random_grid = {\"n_estimators\": n_estimators,\n",
    "               \"max_features\": max_features,\n",
    "               \"min_samples_leaf\": min_samples_leaf,\n",
    "               \"bootstrap\": bootstrap,\n",
    "               \"max_depth\":max_depth}\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest_rs = RandomizedSearchCV(estimator = forest, param_distributions=random_grid, n_iter = 50, cv=5, verbose =2, random_state=42, n_jobs=-1)\n",
    "forest_rs.fit(X_train,y_train)\n",
    "print(forest_rs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_rs_predict = regressor_tester(forest_rs,X_train,y_train,X_val,y_val,y,\"Random Search Random Forest\",False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {\"min_child_weight\": [1,5,10,20,50],\n",
    "          \"gamma\":[0.001,0.01,0.1,1,10,100],\n",
    "          \"subsample\":[0.5,0.75,1],\n",
    "          \"max_depth\":[1,2,3,10,20,30,100,200,300],\n",
    "          \"colsample_bytree\":[0.5,1,2,5,10,20]}\n",
    "\n",
    "xgb = XGBRegressor()\n",
    "xgb_rs = RandomizedSearchCV(xgb,param_distributions=params,n_iter=100,cv=5,n_jobs=-1,verbose=3,random_state=42)\n",
    "xgb_rs.fit(X_train,y_train)\n",
    "print(xgb_rs.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_rs_predict = regressor_tester(xgb_rs,X_train,y_train,X_val,y_val,y,\"Random Search XGB\",False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sgd = SGDRegressor(max_iter=5000,tol=-np.infty, warm_start=True,penalty=None,\n",
    "                             learning_rate=\"constant\",eta0=0.5,early_stopping=True)\n",
    "sgd_predict = regressor_tester(sgd,X_train,y_train,X_val,y_val,y,\"Stochastic Gradient Descent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pg128 polynomial regression \n",
    "\n",
    "poly_features = PolynomialFeatures(degree=5,include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_poly, y, test_size=0.20, random_state=42)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_predict = regressor_tester(lin_reg,X_train,y_train,X_val,y_val,y,\"Polynomial Regression\")"
   ]
  },
  {
   "source": [
    "## After Hyperparameter Tuning "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,X_train,y_train,X_val,y_val = data_dump(df)\n",
    "\n",
    "def baseline_adam_model():\n",
    "    nn = keras.models.Sequential()\n",
    "    nn.add(keras.layers.Dense(30,activation=\"relu\"))\n",
    "    nn.add(keras.layers.Dense(20,activation=\"relu\"))\n",
    "    nn.add(keras.layers.Dense(1))\n",
    "    nn.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n",
    "    return nn\n",
    "def deeper_adam_model():\n",
    "    nn = keras.models.Sequential()\n",
    "    nn.add(keras.layers.Dense(30,activation=\"relu\"))\n",
    "    nn.add(keras.layers.Dense(30,activation=\"relu\"))\n",
    "    nn.add(keras.layers.Dense(30,activation=\"relu\"))\n",
    "    nn.add(keras.layers.Dense(30,activation=\"relu\"))\n",
    "    nn.add(keras.layers.Dense(30,activation=\"relu\"))\n",
    "    nn.add(keras.layers.Dense(1))\n",
    "    nn.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n",
    "    return nn\n",
    "early_stop = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_adam_nn = KerasRegressor(build_fn=baseline_adam_model, epochs=100, batch_size=5,verbose=1)\n",
    "bl_adam_nn.fit(X_train,y_train)\n",
    "y_val_predict_nn = bl_adam_nn.predict(X_val)\n",
    "print(np.sqrt(mean_squared_error(y_val,y_val_predict_nn)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_adam_model = baseline_adam_model()\n",
    "bl_adam_model.fit(X_train,y_train,epochs=100,\n",
    "            validation_data=(X_val,y_val),\n",
    "            callbacks=[early_stop],verbose=1)\n",
    "y_val_predict_nn = bl_adam_model.predict(X_val)\n",
    "print(np.sqrt(mean_squared_error(y_val,y_val_predict_nn)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_adam_nn = KerasRegressor(build_fn=deeper_adam_model, epochs=100, batch_size=5,verbose=1)\n",
    "deep_adam_nn.fit(X_train,y_train)\n",
    "y_val_predict_nn = deep_adam_nn.predict(X_val)\n",
    "print(np.sqrt(mean_squared_error(y_val,y_val_predict_nn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_adam_model = deeper_adam_model()\n",
    "deep_adam_model.fit(X_train,y_train,epochs=100,\n",
    "            validation_data=(X_val,y_val),\n",
    "            callbacks=[early_stop],verbose=1)\n",
    "y_val_predict_nn = deep_adam_model.predict(X_val)\n",
    "print(np.sqrt(mean_squared_error(y_val,y_val_predict_nn)))"
   ]
  },
  {
   "source": [
    "## After Neural Network "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 12,12\n",
    "sns.heatmap(df.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loss(unclean_df,df)\n",
    "X,y,X_train, y_train,X_val,y_val = data_dump(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.price<350]\n",
    "data_loss(unclean_df,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 12,12\n",
    "sns.heatmap(df.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_predict = regressor_tester(lin_reg,X_train,y_train,X_val,y_val,y,\"Linear\")\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest_predict = regressor_tester(forest,X_train,y_train,X_val,y_val,y,\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()\n",
    "df = df.drop([\"number_of_reviews\",\"reviews_per_month\",\"calculated_host_listings_count\",\n",
    "              \"availability_365\",\"neighbourhood\"],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,X_train, y_train,X_val,y_val = data_dump(df)\n",
    "lin_reg = LinearRegression()\n",
    "lin_predict = regressor_tester(lin_reg,X_train,y_train,X_val,y_val,y,\"Linear\")\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest_predict = regressor_tester(forest,X_train,y_train,X_val,y_val,y,\"Random Forest\")"
   ]
  },
  {
   "source": [
    "## Our attemp at overfitting "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "seatle_df = pd.read_csv(\"listings.csv\")\n",
    "# For refrencing later \n",
    "\n",
    "#df[\"reviews_per_month\"] = df[\"reviews_per_month\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seatle_col = list(seatle_df.columns)\n",
    "df_col = list(df.columns)\n",
    "to_del = []\n",
    "for col in seatle_col:\n",
    "    if col not in df_col:\n",
    "        to_del.append(col)\n",
    "to_del.remove(\"room_type\")\n",
    "seatle_df = seatle_df.drop(to_del,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3818 entries, 0 to 3817\nData columns (total 10 columns):\n #   Column                          Non-Null Count  Dtype  \n---  ------                          --------------  -----  \n 0   host_id                         3818 non-null   int64  \n 1   latitude                        3818 non-null   float64\n 2   longitude                       3818 non-null   float64\n 3   room_type                       3818 non-null   object \n 4   price                           3818 non-null   object \n 5   minimum_nights                  3818 non-null   int64  \n 6   availability_365                3818 non-null   int64  \n 7   number_of_reviews               3818 non-null   int64  \n 8   calculated_host_listings_count  3818 non-null   int64  \n 9   reviews_per_month               3818 non-null   float64\ndtypes: float64(3), int64(5), object(2)\nmemory usage: 298.4+ KB\n"
     ]
    }
   ],
   "source": [
    "seatle_df[\"reviews_per_month\"] = seatle_df[\"reviews_per_month\"].fillna(0)\n",
    "\n",
    "seatle_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seatle_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict={}\n",
    "for room in seatle_df['room_type'].tolist():\n",
    "    dict[room]=1\n",
    "    \n",
    "for room in dict.keys():\n",
    "    seatle_df[room] = seatle_df['room_type'].apply(oneHot, hot=room)\n",
    "\n",
    "seatle_df = seatle_df.drop(['room_type'],axis=1)\n",
    "cols = list(seatle_df.columns)\n",
    "cols.sort()\n",
    "seatle_df = seatle_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '85.00'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-47f5968f2338>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"$\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '85.00'"
     ]
    }
   ],
   "source": [
    "X,y,X_train,y_train,X_val,y_val = data_dump(seatle_df)\n",
    "mon = y[0] \n",
    "mon = mon.strip(\"$\")\n",
    "int(mon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "could not convert string to float: '$85.00'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-6b62eded6210>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_dump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseatle_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mseatle_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mRMSE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseatle_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mMAE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseatle_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RMSE = {} MAE = {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRMSE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mMAE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \"\"\"\n\u001b[1;32m--> 255\u001b[1;33m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[0;32m    256\u001b[0m         y_true, y_pred, multioutput)\n\u001b[0;32m    257\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m     84\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    596\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    795\u001b[0m               dtype='datetime64[ns]')\n\u001b[0;32m    796\u001b[0m         \"\"\"\n\u001b[1;32m--> 797\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m     \u001b[1;31m# ----------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\arrays\\numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '$85.00'"
     ]
    }
   ],
   "source": [
    "X,y,X_train,y_train,X_val,y_val = data_dump(seatle_df)\n",
    "seatle_predict = forest.predict(X)\n",
    "RMSE = sqrt(mean_squared_error(y,seatle_predict))\n",
    "MAE = mean_absolute_error(y,seatle_predict)\n",
    "print(\"RMSE = {} MAE = {}\".format(RMSE,MAE))"
   ]
  },
  {
   "source": [
    "## Seatle bby "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}