{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# CS3315 Final Project\n",
    "# Authors: Cameron Woods and Micky Hall \n",
    "\n",
    "## Introduction \n",
    "\n",
    "Throughout this notebook we will be attempting to create a model using supervised learning techniques that can accurately predict the price of an airbnb for a night. We will be using a dataset from kaggle that includes 226,029 rows. Each row represents an individual airbnb listing and includes 15 features and a label which will be discussed in depth at a later point in this notebook. \n",
    "\n",
    "### General Process \n",
    "\n",
    "\n",
    "We began this project by taking all of our data and lightly processing it and attempting to fit it into a bare bones model using linear regression to see how well it would perform. We then analysed the results and looked for reasons in the large skew in our predictions. We then went back and looked at the data as a whole and began to munge our data into a more palatable set for future models. We then checked for any increase in performance from our model. We then began to look into feature engineering and hyper parameter tuning for a greater fit. We slowly worked our way to a better model. We then decided to attempt to run our data in a neural network and see how well that would predict our label. All in all our models ended up predicting with a mse of %%%.   "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "import tensorflow as tf \n",
    "import  tensorflow.keras as keras \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data loading and initial scrub. \n",
    "These features have been dropped because they can either be accurately captured \n",
    "in another feature, we felt they were irrelevant to our prediction, or they were too\n",
    "sparse to be able to munge\n",
    "'''\n",
    "df = pd.read_csv(\"AB_US_2020.csv\")\n",
    "# For refrencing later \n",
    "unclean_df = df \n",
    "\n",
    "df = df.drop([\"name\",\"host_name\",\"city\",\"neighbourhood\",\n",
    "                \"last_review\",\"id\",\"neighbourhood_group\"],axis=1)\n",
    "\n",
    "df[\"reviews_per_month\"] = df[\"reviews_per_month\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "One hot encode the room type feature to be able to represent the differnt type\n",
    "of property you can rent\n",
    "'''\n",
    "def oneHot(category, hot):\n",
    "    if category == hot:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "dict={}\n",
    "for room in df['room_type'].tolist():\n",
    "    dict[room]=1\n",
    "    \n",
    "for room in dict.keys():\n",
    "    df[room] = df['room_type'].apply(oneHot, hot=room)\n",
    "\n",
    "df = df.drop(['room_type'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of our current dataset = 226029\nShape of the original dataset = 226029\nSo we have only dropped 0 rows, which is 0.0 percent of our data\n"
     ]
    }
   ],
   "source": [
    "def data_dump(df):\n",
    "    X = df.drop([\"price\"],axis=1)\n",
    "    y = df[\"price\"]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_val = sc.transform(X_val)\n",
    "    return X,y,X_train,y_train,X_val,y_val\n",
    "def data_loss(original_df, current_df):\n",
    "    orig = original_df.shape[0]\n",
    "    clean = current_df.shape[0]\n",
    "    dropped = orig-clean\n",
    "    perc = (dropped/orig) *100\n",
    "    print(\"Shape of our current dataset = {}\".format(clean))\n",
    "    print(\"Shape of the original dataset = {}\".format(orig))\n",
    "    print(\"So we have only dropped {} rows, which is {} percent of our data\".format(dropped,perc))\n",
    "X,y,X_train,y_train,X_val,y_val = data_dump(df)\n",
    "data_loss(unclean_df,df)"
   ]
  },
  {
   "source": [
    "## Initial Cleaning of Data\n",
    "\n",
    "We began with a dataset that contained the attributes:\n",
    "\n",
    "id, name, host_id, host_name, neighbourhood_group, neighbourhood, latitude, longitude, room_type, price, minimum_nights, number_of_reviews, last_review, reviews_per_month, calculated_host_listings_count, availability_365, and city. \n",
    "\n",
    "After reviewing the data set we decided that it would be best to drop name, host_name, neighbourhood_group, city, neighbourhood, last_review, and id. We dropped name and host_name because they are similar on many dissimilar listings and the value they represent is better caputred in the host_id. We dropped neighbourhood_group, city, and neighbourhood because many of these values were missing, and they can also be represented by the latitude and longitude values given. We dropped last review because it was just the last review of the property which would require us to do some form of semantic analysis to convert to a meaningful attribute. And finally we dropped id because it was just a unique identifier for each listing that held no real value for the models. \n",
    "\n",
    "After dropping these attributes we one hot encoded the room_type attribute so that all property types could be represented, and we filled in all empty values in the reviews_per_month since an empty value is likely to represent no reviews. \n",
    "\n",
    "After dropping and correcting our values we split our labels and features apart and then split them into training and validation sets and then scale them for our models. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor_tester(reg, X_train, y_train, X_val,y_val,y, name,fit=True):\n",
    "    if(fit==True):\n",
    "        reg.fit(X_train, y_train) \n",
    "    y_val_predict = reg.predict(X_val)\n",
    "    RMSE = sqrt(mean_squared_error(y_val, y_val_predict))\n",
    "    MAE = mean_absolute_error(y_val,y_val_predict)\n",
    "    mean = y.describe()[\"mean\"]\n",
    "    std_dev = y.describe()[\"std\"]\n",
    "    print(\"For our {} Regressor the RMSE is {}, MAE is {}\".format(name,RMSE,MAE))\n",
    "    print(\"The current mean value for our label is {} and a single standard deviation is {}\".format(mean,std_dev))\n",
    "    return y_val_predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = LinearRegression()\n",
    "lin_predict = regressor_tester(lin,X_train,y_train,X_val,y_val,y,\"Linear\")\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest_predict = regressor_tester(forest,X_train,y_train,X_val,y_val,y,\"Random Forest\")"
   ]
  },
  {
   "source": [
    "## Initial running of models and light analysis  \n",
    "\n",
    "\n",
    "After lightly cleaning our data we run it through a Linear Regressor and a Random Forrest Regressowithout any hyperparameter tuning and ended up with: \n",
    "\n",
    "RMSE of Linear Regressor = 504.1366\n",
    "RMSE of Random Forest Regressor = 379.4682 \n",
    "\n",
    "When we initially look at this it seems that we are making decent predictions considering we our predicting within 1 standard deviation of error. However, when you look at the data our 75th percentile starts at a price of around 200, so we are grossly over predicting for most of our data. From here we can start to look at the data and munge it some more to try to get better estimates."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\"Linear Actual\":y_val,\"Linear Prediction\":lin_predict})\n",
    "sns.lmplot(x=\"Linear Actual\",y=\"Linear Prediction\",data=pred_df)\n",
    "pred_df = pd.DataFrame({\"Forest Actual\":y_val,\"Forest Prediction\":forest_predict})\n",
    "sns.lmplot(x=\"Forest Actual\",y=\"Forest Prediction\",data=pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 12,12\n",
    "sns.heatmap(df.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x=\"price\",kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of our current dataset = 225940\nShape of the original dataset = 226029\nSo we have only dropped 89 rows, which is 0.03937547836782006 percent of our data\n"
     ]
    }
   ],
   "source": [
    "#drop zeros and negative prices, if any\n",
    "df = df[df.price > 0]\n",
    "#drop highest price, likely an outlier\n",
    "df = df[df.price < 24999]\n",
    "X,y,X_train,y_train,X_val,y_val = data_dump(df)\n",
    "data_loss(unclean_df,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = LinearRegression()\n",
    "lin_predict = regressor_tester(lin,X_train,y_train,X_val,y_val,y,\"Linear\")\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest_predict = regressor_tester(forest,X_train,y_train,X_val,y_val,y,\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\"Linear Actual\":y_val,\"Linear Prediction\":lin_predict})\n",
    "sns.lmplot(x=\"Linear Actual\",y=\"Linear Prediction\",data=pred_df)\n",
    "pred_df = pd.DataFrame({\"Forest Actual\":y_val,\"Forest Prediction\":forest_predict})\n",
    "sns.lmplot(x=\"Forest Actual\",y=\"Forest Prediction\",data=pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop higher priced listings \n",
    "df = df[df.price < 2500]\n",
    "sns.histplot(data=df, x=\"price\",kde=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.price < 1000]\n",
    "sns.histplot(data=df, x=\"price\",kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 12,12\n",
    "sns.heatmap(df.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after having dropped prices > 1000\n",
    "X,y,X_train,y_train,X_val,y_val = data_dump(df)\n",
    "data_loss(unclean_df,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_predict = regressor_tester(lin_reg,X_train,y_train,X_val,y_val,y,\"Linear\")\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest_predict = regressor_tester(forest,X_train,y_train,X_val,y_val,y,\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\"Linear Actual\":y_val,\"Linear Prediction\":lin_predict})\n",
    "sns.lmplot(x=\"Linear Actual\",y=\"Linear Prediction\",data=pred_df)\n",
    "pred_df = pd.DataFrame({\"Forest Actual\":y_val,\"Forest Prediction\":forest_predict})\n",
    "sns.lmplot(x=\"Forest Actual\",y=\"Forest Prediction\",data=pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_drops = df \n",
    "df = df.drop([\"minimum_nights\"],axis=1)\n",
    "X,y,X_train,y_train,X_val,y_val = data_dump(df)\n",
    "data_loss(unclean_df,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_predict = regressor_tester(lin_reg,X_train,y_train,X_val,y_val,y,\"Linear\")\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest_predict = regressor_tester(forest,X_train,y_train,X_val,y_val,y,\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"host_id\"],axis=1)\n",
    "X,y,X_train,y_train,X_val,y_val = data_dump(df)\n",
    "data_loss(unclean_df,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_predict = regressor_tester(lin_reg,X_train,y_train,X_val,y_val,y,\"Linear\")\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest_predict = regressor_tester(forest,X_train,y_train,X_val,y_val,y,\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.price < 600]\n",
    "df = df.drop([\"Hotel room\"],axis=1)\n",
    "X,y,X_train,y_train,X_val,y_val = data_dump(df)\n",
    "data_loss(unclean_df,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_predict = regressor_tester(lin_reg,X_train,y_train,X_val,y_val,y,\"Linear\")\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest_predict = regressor_tester(forest,X_train,y_train,X_val,y_val,y,\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\"Linear Actual\":y_val,\"Linear Prediction\":lin_predict})\n",
    "sns.lmplot(x=\"Linear Actual\",y=\"Linear Prediction\",data=pred_df)\n",
    "pred_df = pd.DataFrame({\"Forest Actual\":y_val,\"Forest Prediction\":forest_predict})\n",
    "sns.lmplot(x=\"Forest Actual\",y=\"Forest Prediction\",data=pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert neighbourhood into label encoded featuresS\n",
    "labelencoder = LabelEncoder()\n",
    "df[\"neighbourhood\"] = unclean_df[\"neighbourhood\"]\n",
    "df[\"neighbourhood\"] = labelencoder.fit_transform(df[\"neighbourhood\"])\n",
    "X,y,X_train,y_train,X_val,y_val = data_dump(df)\n",
    "data_loss(unclean_df,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_predict = regressor_tester(lin_reg,X_train,y_train,X_val,y_val,y,\"Linear\")\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest_predict = regressor_tester(forest,X_train,y_train,X_val,y_val,y,\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\"Linear Actual\":y_val,\"Linear Prediction\":lin_predict})\n",
    "sns.lmplot(x=\"Linear Actual\",y=\"Linear Prediction\",data=pred_df)\n",
    "pred_df = pd.DataFrame({\"Forest Actual\":y_val,\"Forest Prediction\":forest_predict})\n",
    "sns.lmplot(x=\"Forest Actual\",y=\"Forest Prediction\",data=pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = before_drops \n",
    "df = df[df.price < 0]"
   ]
  },
  {
   "source": [
    "## After Data Analysis "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_estimators = list(range(10,101,10))\n",
    "max_features = [\"log2\",\"auto\",\"sqrt\"]\n",
    "min_samples_leaf = [1,2,4,6]\n",
    "bootstrap = [False,True]\n",
    "max_depth = [100,150,200,250,300,None]\n",
    "random_grid = {\"n_estimators\": n_estimators,\n",
    "               \"max_features\": max_features,\n",
    "               \"min_samples_leaf\": min_samples_leaf,\n",
    "               \"bootstrap\": bootstrap,\n",
    "               \"max_depth\":max_depth}\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest_rs = RandomizedSearchCV(estimator = forest, param_distributions=random_grid, n_iter = 50, cv=5, verbose =2, random_state=42, n_jobs=-1)\n",
    "forest_rs.fit(X_train,y_train)\n",
    "print(forest_rs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_rs_predict = regressor_tester(forest_rs,X_train,y_train,X_val,y_val,y,\"Random Search Random Forest\",False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {\"min_child_weight\": [1,5,10,20,50],\n",
    "          \"gamma\":[0.001,0.01,0.1,1,10,100],\n",
    "          \"subsample\":[0.5,0.75,1],\n",
    "          \"max_depth\":[1,2,3,10,20,30,100,200,300],\n",
    "          \"colsample_bytree\":[0.5,1,2,5,10,20]}\n",
    "\n",
    "xgb = XGBRegressor()\n",
    "xgb_rs = RandomizedSearchCV(xgb,param_distributions=params,n_iter=100,cv=5,n_jobs=-1,verbose=3,random_state=42)\n",
    "xgb_rs.fit(X_train,y_train)\n",
    "print(xgb_rs.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_rs_predict = regressor_tester(xgb_rs,X_train,y_train,X_val,y_val,y,\"Random Search XGB\",False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sgd = SGDRegressor(max_iter=5000,tol=-np.infty, warm_start=True,penalty=None,\n",
    "                             learning_rate=\"constant\",eta0=0.5,early_stopping=True)\n",
    "sgd_predict = regressor_tester(sgd,X_train,y_train,X_val,y_val,y,\"Stochastic Gradient Descent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pg128 polynomial regression \n",
    "\n",
    "poly_features = PolynomialFeatures(degree=5,include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_poly, y, test_size=0.20, random_state=42)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_predict = regressor_tester(lin_reg,X_train,y_train,X_val,y_val,y,\"Polynomial Regression\")"
   ]
  },
  {
   "source": [
    "## After Hyperparameter Tuning "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,X_train,y_train,X_val,y_val = data_dump(df)\n",
    "\n",
    "def baseline_adam_model():\n",
    "    nn = keras.models.Sequential()\n",
    "    nn.add(keras.layers.Dense(30,activation=\"relu\"))\n",
    "    nn.add(keras.layers.Dense(20,activation=\"relu\"))\n",
    "    nn.add(keras.layers.Dense(1))\n",
    "    nn.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n",
    "    return nn\n",
    "def deeper_adam_model():\n",
    "    nn = keras.models.Sequential()\n",
    "    nn.add(keras.layers.Dense(30,activation=\"relu\"))\n",
    "    nn.add(keras.layers.Dense(30,activation=\"relu\"))\n",
    "    nn.add(keras.layers.Dense(30,activation=\"relu\"))\n",
    "    nn.add(keras.layers.Dense(30,activation=\"relu\"))\n",
    "    nn.add(keras.layers.Dense(30,activation=\"relu\"))\n",
    "    nn.add(keras.layers.Dense(1))\n",
    "    nn.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n",
    "    return nn\n",
    "early_stop = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_adam_model = baseline_adam_model()\n",
    "bl_adam_model.fit(X_train,y_train,epochs=100,\n",
    "            validation_data=(X_val,y_val),\n",
    "            callbacks=[early_stop],verbose=1)\n",
    "baseline_predict = bl_adam_model.predict(X_val)\n",
    "print(np.sqrt(mean_squared_error(y_val,baseline_predict)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_adam_model = deeper_adam_model()\n",
    "deep_adam_model.fit(X_train,y_train,epochs=100,\n",
    "            validation_data=(X_val,y_val),\n",
    "            callbacks=[early_stop],verbose=1)\n",
    "deep_predict = deep_adam_model.predict(X_val)\n",
    "print(np.sqrt(mean_squared_error(y_val,deep_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\"Baseline Actual\":y_val,\"Baseline Prediction\":baseline_predict})\n",
    "sns.lmplot(x=\"Baseline Actual\",y=\"Baseline Prediction\",data=pred_df)\n",
    "pred_df = pd.DataFrame({\"Deep Actual\":y_val,\"Deep Prediction\":forest_predict})\n",
    "sns.lmplot(x=\"Deep Actual\",y=\"Deep Prediction\",data=pred_df)"
   ]
  },
  {
   "source": [
    "## After Neural Network "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 12,12\n",
    "sns.heatmap(df.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loss(unclean_df,df)\n",
    "X,y,X_train, y_train,X_val,y_val = data_dump(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of our current dataset = 181738\nShape of the original dataset = 226029\nSo we have only dropped 44291 rows, which is 19.59527317291144 percent of our data\n"
     ]
    }
   ],
   "source": [
    "#df = before_drops\n",
    "df = df[df.price<250]\n",
    "data_loss(unclean_df,df)\n",
    "X,y,X_train, y_train,X_val,y_val = data_dump(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 12,12\n",
    "sns.heatmap(df.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For our Linear Regressor the RMSE is 45.83067753013744, MAE is 36.5179895887847\n",
      "The current mean value for our label is 111.04913116684457 and a single standard deviation is 54.98028813327663\n",
      "For our Random Forest Regressor the RMSE is 37.969223734956, MAE is 28.52584627234437\n",
      "The current mean value for our label is 111.04913116684457 and a single standard deviation is 54.98028813327663\n"
     ]
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_predict = regressor_tester(lin_reg,X_train,y_train,X_val,y_val,y,\"Linear\")\n",
    "\n",
    "forest = RandomForestRegressor(n_jobs=-1)\n",
    "forest_predict = regressor_tester(forest,X_train,y_train,X_val,y_val,y,\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()\n",
    "df = df.drop([\"number_of_reviews\",\"reviews_per_month\",\"calculated_host_listings_count\",\n",
    "              \"availability_365\",\"neighbourhood\"],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,X_train, y_train,X_val,y_val = data_dump(df)\n",
    "lin_reg = LinearRegression()\n",
    "lin_predict = regressor_tester(lin_reg,X_train,y_train,X_val,y_val,y,\"Linear\")\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest_predict = regressor_tester(forest,X_train,y_train,X_val,y_val,y,\"Random Forest\")"
   ]
  },
  {
   "source": [
    "## Our attemp at overfitting "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For our Forest for Seattle Regressor the RMSE is 38.022100513403025, MAE is 28.56648437402314\nThe current mean value for our label is 111.04913116684457 and a single standard deviation is 54.98028813327663\n"
     ]
    }
   ],
   "source": [
    "# Create a data set that is comparable to the new csv \n",
    "df_for_seatle = df.drop([\"Hotel room\"],axis=1)\n",
    "cols = list(df_for_seatle.columns)\n",
    "cols.sort()\n",
    "df_for_seatle = df_for_seatle[cols]\n",
    "X,y,X_train,y_train,X_val,y_val = data_dump(df_for_seatle)\n",
    "\n",
    "forest = RandomForestRegressor(n_jobs=-1)\n",
    "forest_predict = regressor_tester(forest,X_train,y_train,X_val,y_val,y,\"Forest for Seattle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import new data, fix missing data\n",
    "seatle_df = pd.read_csv(\"listings.csv\")\n",
    "\n",
    "seatle_df[\"reviews_per_month\"] = seatle_df[\"reviews_per_month\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of columns not in original data set \n",
    "seatle_col = list(seatle_df.columns)\n",
    "df_col = list(df_for_seatle.columns)\n",
    "to_del = []\n",
    "for col in seatle_col:\n",
    "    if col not in df_col:\n",
    "        to_del.append(col)\n",
    "to_del.remove(\"room_type\")\n",
    "seatle_df = seatle_df.drop(to_del,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode the room type column like in our original data \n",
    "dict={}\n",
    "for room in seatle_df['room_type'].tolist():\n",
    "    dict[room]=1\n",
    "    \n",
    "for room in dict.keys():\n",
    "    seatle_df[room] = seatle_df['room_type'].apply(oneHot, hot=room)\n",
    "\n",
    "seatle_df = seatle_df.drop(['room_type'],axis=1)\n",
    "cols = list(seatle_df.columns)\n",
    "cols.sort()\n",
    "seatle_df = seatle_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the ys from strings to ints \n",
    "from re import sub\n",
    "from decimal import Decimal\n",
    "X,y,X_train,y_train,X_val,y_val = data_dump(seatle_df)\n",
    "for i in range(len(y)):\n",
    "    y[i] = float(Decimal(sub(r'[^\\d.]', '', y[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE = 91.44324877232155 MAE = 66.02685201238647\n"
     ]
    }
   ],
   "source": [
    "# use our model to see how accurate our predictions are \n",
    "seatle_predict = forest.predict(X)\n",
    "RMSE = sqrt(mean_squared_error(y,seatle_predict))\n",
    "MAE = mean_absolute_error(y,seatle_predict)\n",
    "print(\"RMSE = {} MAE = {}\".format(RMSE,MAE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the current standard deviation of our seatle dataset is 90.23820236938388\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "ly = list(y)\n",
    "ly.sort()\n",
    "stddev = statistics.pstdev(ly)\n",
    "print(\"the current standard deviation of our seatle dataset is {}\".format(stddev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new df from our original one that only contains Seatle \n",
    "df_for_seatle = unclean_df[unclean_df.city ==\"Seattle\"]\n",
    "df_for_seatle = df_for_seatle.drop([\"name\",\"host_name\",\"city\",\"neighbourhood\",\n",
    "                \"last_review\",\"id\",\"neighbourhood_group\"],axis=1)\n",
    "\n",
    "df_for_seatle[\"reviews_per_month\"] = df_for_seatle[\"reviews_per_month\"].fillna(0)\n",
    "\n",
    "dict={}\n",
    "for room in df_for_seatle['room_type'].tolist():\n",
    "    dict[room]=1\n",
    "    \n",
    "for room in dict.keys():\n",
    "    df_for_seatle[room] = df_for_seatle['room_type'].apply(oneHot, hot=room)\n",
    "\n",
    "df_for_seatle = df_for_seatle.drop(['room_type'],axis=1)\n",
    "\n",
    "df_for_seatle = df_for_seatle.drop([\"Hotel room\"],axis=1)\n",
    "df_for_seatle = df_for_seatle[df_for_seatle.price < 250]\n",
    "df_for_seatle = df_for_seatle[df_for_seatle.price > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For our Forest for only Seattle Regressor the RMSE is 38.33285265343837, MAE is 28.779356956521735\nThe current mean value for our label is 112.76409185803757 and a single standard deviation is 52.23616265056708\n"
     ]
    }
   ],
   "source": [
    "# Train the model \n",
    "X,y,X_train,y_train,X_val,y_val = data_dump(df_for_seatle)\n",
    "\n",
    "forest = RandomForestRegressor(n_jobs=-1)\n",
    "forest_predict = regressor_tester(forest,X_train,y_train,X_val,y_val,y,\"Forest for only Seattle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0        85\n1       150\n2       975\n3       100\n4       450\n       ... \n3813    359\n3814     79\n3815     93\n3816     99\n3817     87\nName: price, Length: 3818, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Transform the ys from strings to ints \n",
    "from re import sub\n",
    "from decimal import Decimal\n",
    "X,y,X_train,y_train,X_val,y_val = data_dump(seatle_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE = 112.99960164450893 MAE = 97.07481927710843\n"
     ]
    }
   ],
   "source": [
    "seatle_predict = forest.predict(X)\n",
    "RMSE = sqrt(mean_squared_error(y,seatle_predict))\n",
    "MAE = mean_absolute_error(y,seatle_predict)\n",
    "print(\"RMSE = {} MAE = {}\".format(RMSE,MAE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Standard deviation for our new data set is 90.23820236938388\n"
     ]
    }
   ],
   "source": [
    "ly = list(y)\n",
    "ly.sort()\n",
    "import statistics\n",
    "stddev = statistics.pstdev(ly)\n",
    "print(\"Standard deviation for our new data set is {}\".format(stddev))"
   ]
  },
  {
   "source": [
    "## Seatle bby "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}